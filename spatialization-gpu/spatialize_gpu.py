import json
import numpy as np
import pandas as pd
from cudf import DataFrame
from cuml.preprocessing import OneHotEncoder
from cuml.manifold import UMAP
from datetime import datetime
from tqdm import tqdm
from time import time
from cuml.metrics import pairwise_distances
import os
from pathlib import Path

# Chemins des fichiers
INPUT_PATH = Path("../client/public/data/posts.json")
OUTPUT_DIR = Path("../client/public/data")

# Configuration optimisÃ©e pour GPU
UMAP_PARAMS = {
    'n_neighbors': 1000,    # Plus de voisins pour une meilleure connexion globale
    'min_dist': 0.05,       # Distance minimale entre les points
    'spread': 0.8,          # Distribution plus Ã©talÃ©e
    'n_components': 3,      # Dimensions de sortie
    'random_state': 42
}

# Poids des diffÃ©rentes composantes
WEIGHTS = {
    'sourceType': 0.33,
    'thematic': 0.33,
    'character': 0.34
}

def print_time_estimate(start_time, current_step, total_steps):
    """Affiche une estimation du temps restant."""
    elapsed = time() - start_time
    estimated_total = (elapsed / current_step) * total_steps
    remaining = estimated_total - elapsed
    print(f"â±ï¸  Temps Ã©coulÃ© : {elapsed:.2f}s | Temps restant estimÃ© : {remaining:.2f}s")

def clean_text(text):
    """Nettoie le texte."""
    if not isinstance(text, str):
        return ""
    text = text.lower()
    return ' '.join(text.split())

def load_data(file_path):
    """Charge et prÃ©pare les donnÃ©es des posts."""
    print("\nğŸ“š Chargement des donnÃ©es...")
    start_time = time()
    
    with open(file_path, 'r', encoding='utf-8') as f:
        posts = json.load(f)
    
    # PrÃ©paration des donnÃ©es
    processed_posts = []
    for post in tqdm(posts, desc="Traitement des posts"):
        processed_posts.append({
            'id': post.get('id', ''),
            'sourceType': post.get('sourceType', 'unknown'),
            'thematic': post.get('thematic', 'unknown'),
            'character': post.get('character', 'unknown'),
            'original': post
        })
    
    print(f"âœ… {len(processed_posts)} posts traitÃ©s")
    print(f"â±ï¸  Temps de traitement : {time() - start_time:.2f}s")
    return processed_posts

def vectorize_features_gpu(posts):
    """Vectorise les mÃ©tadonnÃ©es avec accÃ©lÃ©ration GPU."""
    print("\nğŸ”¤ Vectorisation des donnÃ©es sur GPU...")
    start_time = time()
    
    # CrÃ©ation du DataFrame RAPIDS
    df = DataFrame({
        'sourceType': [post['sourceType'] for post in posts],
        'thematic': [post['thematic'] for post in posts],
        'character': [post['character'] for post in posts]
    })
    
    # Encodage des mÃ©tadonnÃ©es catÃ©gorielles
    print("   Encodage des mÃ©tadonnÃ©es...")
    categorical_vectors = {}
    
    for feature in ['sourceType', 'thematic', 'character']:
        # Utilisation de get_dummies de cudf pour one-hot encoding
        feature_vectors = DataFrame.get_dummies(df[feature], prefix=feature)
        categorical_vectors[feature] = feature_vectors
        
        # Afficher les catÃ©gories uniques
        unique_values = df[feature].unique().values_host
        print(f"\nğŸ“Š CatÃ©gories pour {feature}:")
        for val in unique_values:
            count = (df[feature] == val).sum()
            print(f"   - {val}: {count} posts")
    
    # Combinaison des vecteurs avec les poids
    print("\n   Combinaison des vecteurs...")
    weighted_vectors = []
    
    for feature in ['sourceType', 'thematic', 'character']:
        feature_vectors = categorical_vectors[feature]
        if len(feature_vectors.columns) > 0:
            feature_vectors_normalized = feature_vectors / feature_vectors.max() if feature_vectors.max().any() else feature_vectors
            weighted_vectors.append(WEIGHTS[feature] * feature_vectors_normalized)
    
    # Combiner tous les vecteurs
    combined_vectors = pd.concat([v.to_pandas() for v in weighted_vectors], axis=1)
    
    # Calcul de similaritÃ©s sur un Ã©chantillon
    sample_size = min(1000, combined_vectors.shape[0])
    sample_vectors = combined_vectors.iloc[:sample_size]
    similarities = pairwise_distances(sample_vectors, metric='cosine')
    avg_similarity = float(np.mean(similarities[np.eye(sample_size, dtype=bool) == 0]))
    
    print(f"\nâœ… Dimension des vecteurs finaux : {combined_vectors.shape}")
    print(f"ğŸ“Š SimilaritÃ© moyenne entre les posts : {avg_similarity:.3f}")
    print(f"â±ï¸  Temps de vectorisation : {time() - start_time:.2f}s")
    
    return combined_vectors

def apply_umap_gpu(vectors):
    """Applique UMAP avec accÃ©lÃ©ration GPU."""
    print("\nğŸŒ Application d'UMAP sur GPU...")
    start_time = time()
    
    # Initialisation de UMAP optimisÃ© GPU
    umap_model = UMAP(**UMAP_PARAMS)
    
    with tqdm(total=100, desc="Progression UMAP") as pbar:
        def update_progress(progress):
            increment = progress - pbar.n
            if increment > 0:
                pbar.update(increment)
        
        coordinates = umap_model.fit_transform(vectors)
    
    # Calcul des statistiques
    distances = pairwise_distances(coordinates, metric='euclidean')
    avg_distance = float(np.mean(distances[distances > 0]))
    max_distance = float(np.max(distances))
    
    print(f"ğŸ“Š Distance moyenne entre les points : {avg_distance:.3f}")
    print(f"ğŸ“Š Distance maximale entre les points : {max_distance:.3f}")
    print(f"â±ï¸  Temps de rÃ©duction dimensionnelle : {time() - start_time:.2f}s")
    
    return coordinates

def save_results(posts, coordinates, output_file):
    """Sauvegarde les rÃ©sultats avec les coordonnÃ©es."""
    print("\nğŸ’¾ Sauvegarde des rÃ©sultats...")
    start_time = time()
    results = []
    
    for post, coords in tqdm(zip(posts, coordinates), total=len(posts), desc="PrÃ©paration des donnÃ©es"):
        spatialized_post = post['original'].copy()
        spatialized_post['coordinates'] = {
            'x': float(coords[0]),
            'y': float(coords[1]),
            'z': float(coords[2])
        }
        results.append(spatialized_post)
    
    # CrÃ©ation du nom de fichier avec les paramÃ¨tres
    output_filename = f"spatialized_posts_gpu_n-{UMAP_PARAMS['n_neighbors']}_d-{UMAP_PARAMS['min_dist']}_s-{UMAP_PARAMS['spread']}.json"
    output_path = OUTPUT_DIR / output_filename
    
    # CrÃ©er le dossier de sortie s'il n'existe pas
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2)
    
    print(f"âœ… RÃ©sultats sauvegardÃ©s dans : {output_path}")
    print(f"â±ï¸  Temps de sauvegarde : {time() - start_time:.2f}s")
    return output_path

def main():
    total_start_time = time()
    
    # VÃ©rifier que le fichier d'entrÃ©e existe
    if not INPUT_PATH.exists():
        raise FileNotFoundError(f"Le fichier d'entrÃ©e n'existe pas : {INPUT_PATH}")
    
    # Charge les donnÃ©es
    posts = load_data(INPUT_PATH)
    
    # Vectorise les features sur GPU
    vectors = vectorize_features_gpu(posts)
    
    # Applique UMAP sur GPU
    coordinates = apply_umap_gpu(vectors)
    
    # Sauvegarde les rÃ©sultats
    output_path = save_results(posts, coordinates, 'spatialized_posts.json')
    
    total_time = time() - total_start_time
    print(f"\nğŸ‰ Spatialisation GPU terminÃ©e en {total_time:.2f} secondes !")
    print(f"ğŸ“ RÃ©sultats disponibles dans : {output_path}")

if __name__ == "__main__":
    main() 