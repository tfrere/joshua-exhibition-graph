import json
import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from umap import UMAP
from datetime import datetime
from tqdm import tqdm
from time import time
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse import hstack
import os
import argparse
import re
from pathlib import Path

# Chemins des fichiers
INPUT_PATH = Path("./data/output/decimated-posts-with-positions.json")
OUTPUT_DIR = Path("./data/output")

# Configuration
UMAP_PARAMS = {
    'n_neighbors': 120,    # encore plus de voisins pour une meilleure connexion globale
    'min_dist': 8,     # un peu plus proche mais pas trop
    'spread': 8,        # distribution plus √©tal√©e pour faciliter la navigation
    'n_components': 3,
    'random_state': 42
}

# Poids des diff√©rentes composantes dans la vectorisation finale (valeurs par d√©faut)
DEFAULT_DIMENSIONS = ['source', 'thematic', 'slug']

def print_time_estimate(start_time, current_step, total_steps):
    """Affiche une estimation du temps restant."""
    elapsed = time() - start_time
    estimated_total = (elapsed / current_step) * total_steps
    remaining = estimated_total - elapsed
    print(f"Temps √©coul√© : {elapsed:.2f}s | Temps restant estim√© : {remaining:.2f}s")

def clean_text(text):
    """Nettoie le texte en supprimant les URLs, les caract√®res sp√©ciaux, etc."""
    if not isinstance(text, str):
        return ""
    # Supprime les URLs
    text = re.sub(r'http\S+', '', text)
    # Supprime les r√©f√©rences 4chan
    text = re.sub(r'\/\w+\/', ' ', text)
    text = re.sub(r'ID:\w+', ' ', text)
    # Supprime les caract√®res sp√©ciaux
    text = re.sub(r'[^\w\s]', ' ', text)
    # Normalise les espaces
    text = ' '.join(text.split())
    return text.lower()

def load_data(file_path):
    """Charge et pr√©pare les donn√©es des posts."""
    print("\nüìö Chargement des donn√©es...")
    start_time = time()
    
    with open(file_path, 'r', encoding='utf-8') as f:
        posts = json.load(f)
    
    # Pr√©pare les donn√©es pour la vectorisation
    processed_posts = []
    for post in tqdm(posts, desc="Traitement des posts"):
        processed_post = {
            'id': post.get('id', ''),
            'original': post
        }
        
        # Ajoute toutes les propri√©t√©s disponibles pour une utilisation ult√©rieure
        for key, value in post.items():
            if key != 'id':
                processed_post[key] = post.get(key, 'unknown')
                
        processed_posts.append(processed_post)
    
    print(f"‚úÖ {len(processed_posts)} posts trait√©s")
    print(f"‚è±Ô∏è  Temps de traitement : {time() - start_time:.2f}s")
    return processed_posts

def vectorize_features(posts, dimensions, custom_weights=None):
    """Vectorise les m√©tadonn√©es selon les dimensions sp√©cifi√©es."""
    print("\nüî§ Vectorisation des donn√©es...")
    start_time = time()
    
    # V√©rifier que les dimensions existent dans les donn√©es
    available_keys = set()
    for post in posts[:10]:  # V√©rifier les 10 premiers posts pour obtenir les cl√©s disponibles
        available_keys.update(post.keys())
    
    available_keys.discard('id')
    available_keys.discard('original')
    
    valid_dimensions = []
    for dim in dimensions:
        if dim in available_keys:
            valid_dimensions.append(dim)
        else:
            print(f"‚ö†Ô∏è Dimension '{dim}' non trouv√©e dans les donn√©es. Elle sera ignor√©e.")
    
    if not valid_dimensions:
        print("‚ùå Aucune dimension valide sp√©cifi√©e. Utilisation des dimensions par d√©faut.")
        valid_dimensions = [dim for dim in DEFAULT_DIMENSIONS if dim in available_keys]
        if not valid_dimensions:
            raise ValueError("Aucune dimension valide disponible pour la vectorisation.")
    
    print(f"üîç Dimensions utilis√©es pour la vectorisation : {valid_dimensions}")
    
    # Calculer les poids pour chaque dimension
    if custom_weights and len(custom_weights) == len(valid_dimensions):
        # Utiliser les poids personnalis√©s fournis
        weights = {dim: float(weight) for dim, weight in zip(valid_dimensions, custom_weights)}
        print(f"‚öñÔ∏è Poids personnalis√©s utilis√©s : {weights}")
    else:
        if custom_weights:
            print(f"‚ö†Ô∏è Le nombre de poids ({len(custom_weights)}) ne correspond pas au nombre de dimensions valides ({len(valid_dimensions)}). Utilisation de poids √©quilibr√©s.")
        # R√©partition √©gale des poids
        weights = {dim: 1.0 / len(valid_dimensions) for dim in valid_dimensions}
        print(f"‚öñÔ∏è Poids √©quilibr√©s g√©n√©r√©s : {weights}")
    
    # Normaliser les poids pour qu'ils somment √† 1
    total_weight = sum(weights.values())
    if total_weight != 1.0:
        weights = {dim: weight / total_weight for dim, weight in weights.items()}
        print(f"‚öñÔ∏è Poids normalis√©s : {weights}")
    
    # Encodage des m√©tadonn√©es cat√©gorielles
    print("   Encodage des m√©tadonn√©es...")
    categorical_encoders = {}
    categorical_vectors = {}
    
    for feature in valid_dimensions:
        encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')
        feature_values = [[post.get(feature, 'unknown')] for post in posts]
        feature_vectors = encoder.fit_transform(feature_values)
        categorical_encoders[feature] = encoder
        categorical_vectors[feature] = feature_vectors
        
        # Afficher les cat√©gories uniques pour chaque feature
        unique_values = encoder.categories_[0]
        print(f"\nüìä Cat√©gories pour {feature}:")
        for val in unique_values:
            count = sum(1 for post in posts if post.get(feature, 'unknown') == val)
            print(f"   - {val}: {count} posts")
    
    # Combinaison des vecteurs avec les poids
    print("\n   Combinaison des vecteurs...")
    weighted_vectors = []
    
    for feature in valid_dimensions:
        feature_vectors = categorical_vectors[feature]
        if feature_vectors.shape[1] > 0:  # Si des cat√©gories ont √©t√© trouv√©es
            feature_vectors_normalized = feature_vectors / feature_vectors.max() if feature_vectors.max() > 0 else feature_vectors
            weighted_vectors.append(weights[feature] * feature_vectors_normalized)
    
    # Combiner tous les vecteurs
    combined_vectors = hstack(weighted_vectors)
    
    # Calcul de quelques statistiques
    sample_size = min(1000, combined_vectors.shape[0])
    similarities = cosine_similarity(combined_vectors[:sample_size], combined_vectors[:sample_size])
    avg_similarity = np.mean(similarities[similarities != 1])
    
    print(f"\n‚úÖ Dimension des vecteurs finaux : {combined_vectors.shape}")
    print(f"üìä Similarit√© moyenne entre les posts : {avg_similarity:.3f}")
    print(f"‚è±Ô∏è  Temps de vectorisation : {time() - start_time:.2f}s")
    
    return combined_vectors, valid_dimensions, weights

def apply_umap(vectors):
    """Applique UMAP pour la r√©duction dimensionnelle."""
    print("\nüåê Application d'UMAP...")
    start_time = time()
    
    umap_model = UMAP(**UMAP_PARAMS)
    with tqdm(total=100, desc="Progression UMAP") as pbar:
        def update_progress(progress):
            increment = progress - pbar.n
            if increment > 0:
                pbar.update(increment)
        
        umap_model._handle_progress = update_progress
        coordinates = umap_model.fit_transform(vectors.toarray())
    
    # Calcul de quelques statistiques sur les coordonn√©es
    distances = np.sqrt(np.sum((coordinates[:, None] - coordinates) ** 2, axis=2))
    avg_distance = np.mean(distances[distances > 0])
    max_distance = np.max(distances)
    
    print(f"üìä Distance moyenne entre les points : {avg_distance:.3f}")
    print(f"üìä Distance maximale entre les points : {max_distance:.3f}")
    print(f"‚è±Ô∏è  Temps de r√©duction dimensionnelle : {time() - start_time:.2f}s")
    return coordinates

def save_results(posts, coordinates, dimensions, weights, output_file):
    """Sauvegarde les r√©sultats avec les coordonn√©es."""
    print("\nüíæ Sauvegarde des r√©sultats...")
    start_time = time()
    results = []
    
    for post, coords in tqdm(zip(posts, coordinates), total=len(posts), desc="Pr√©paration des donn√©es"):
        spatialized_post = post['original'].copy()
        spatialized_post['coordinates'] = {
            'x': float(coords[0]),
            'y': float(coords[1]),
            'z': float(coords[2])
        }
        # Ajouter des m√©tadonn√©es sur la spatialisation
        spatialized_post['_spatialMetadata'] = {
            'dimensions': dimensions,
            'weights': weights,
            'umap_params': UMAP_PARAMS
        }
        results.append(spatialized_post)
    
    # Cr√©ation du nom de fichier avec les param√®tres et dimensions
    dimensions_str = "-".join(dimensions)
    output_filename = f"spatialized_posts_dim-{dimensions_str}_n-{UMAP_PARAMS['n_neighbors']}_d-{UMAP_PARAMS['min_dist']}_s-{UMAP_PARAMS['spread']}.json"
    output_path = OUTPUT_DIR / output_filename

    output_filename2 = "spatialized_posts.json"
    output_path2 = OUTPUT_DIR / output_filename2
    
    # Cr√©er le dossier de sortie s'il n'existe pas
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2)
    
    with open(output_path2, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2)
    

    print(f"‚úÖ R√©sultats sauvegard√©s dans : {output_path}")
    print(f"‚úÖ et une copie dans : {output_path2}")
    print(f"‚è±Ô∏è  Temps de sauvegarde : {time() - start_time:.2f}s")
    return output_path

def main():
    # Configurer l'analyse des arguments de ligne de commande
    global INPUT_PATH, OUTPUT_DIR, UMAP_PARAMS
    
    parser = argparse.ArgumentParser(description='Spatialisation des posts avec UMAP')
    parser.add_argument('--input', type=str, default=str(INPUT_PATH),
                        help='Chemin vers le fichier de posts')
    parser.add_argument('--output-dir', type=str, default=str(OUTPUT_DIR),
                        help='Dossier de sortie pour les r√©sultats')
    parser.add_argument('--dimensions', type=str, nargs='+', default=DEFAULT_DIMENSIONS,
                        help='Dimensions √† utiliser pour la vectorisation, ex: source thematic slug')
    parser.add_argument('--weights', type=float, nargs='+', default=None,
                        help='Poids √† appliquer √† chaque dimension (doit √™tre le m√™me nombre que --dimensions)')
    parser.add_argument('--n-neighbors', type=int, default=UMAP_PARAMS['n_neighbors'],
                        help='Nombre de voisins pour UMAP')
    parser.add_argument('--min-dist', type=float, default=UMAP_PARAMS['min_dist'],
                        help='Distance minimale pour UMAP')
    parser.add_argument('--spread', type=float, default=UMAP_PARAMS['spread'],
                        help='Spread pour UMAP')
    
    args = parser.parse_args()
    
    # Mettre √† jour les param√®tres
    INPUT_PATH = Path(args.input)
    OUTPUT_DIR = Path(args.output_dir)
    UMAP_PARAMS['n_neighbors'] = args.n_neighbors
    UMAP_PARAMS['min_dist'] = args.min_dist
    UMAP_PARAMS['spread'] = args.spread
    
    total_start_time = time()
    
    # Afficher la configuration
    print("\nüîß Configuration:")
    print(f"   - Fichier d'entr√©e: {INPUT_PATH}")
    print(f"   - Dossier de sortie: {OUTPUT_DIR}")
    print(f"   - Dimensions: {args.dimensions}")
    if args.weights:
        print(f"   - Poids: {args.weights}")
    print(f"   - Param√®tres UMAP: n_neighbors={UMAP_PARAMS['n_neighbors']}, min_dist={UMAP_PARAMS['min_dist']}, spread={UMAP_PARAMS['spread']}")
    
    # V√©rifier que le fichier d'entr√©e existe
    if not INPUT_PATH.exists():
        raise FileNotFoundError(f"Le fichier d'entr√©e n'existe pas : {INPUT_PATH}")
    
    # Charge les donn√©es
    posts = load_data(INPUT_PATH)
    
    # Vectorise les features avec les dimensions sp√©cifi√©es
    vectors, valid_dimensions, weights = vectorize_features(posts, args.dimensions, args.weights)
    
    # Applique UMAP
    coordinates = apply_umap(vectors)
    
    # Sauvegarde les r√©sultats
    output_path = save_results(posts, coordinates, valid_dimensions, weights, 'spatialized_posts.json')
    
    total_time = time() - total_start_time
    print(f"\nüéâ Spatialisation termin√©e en {total_time:.2f} secondes !")
    print(f"üìÅ R√©sultats disponibles dans : {output_path}")

if __name__ == "__main__":
    main() 